{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from prepare_data import prepare\n",
    "from load_data import load\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# import catboost as ctb\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [00:08<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "train, test, y, train_dict = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = prepare(pd.concat([train, test]).reset_index(drop=True), train_dict)\n",
    "\n",
    "train = all_data.loc[:train.shape[0] -1,:]\n",
    "test = all_data.loc[train.shape[0]:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y, scoring='neg_mean_squared_error', cv=kf, n_jobs=-1))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state=7, nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=1000,\n",
    "                              max_bin=55, bagging_fraction=0.8,\n",
    "                              bagging_freq=5, feature_fraction=0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf=6, min_sum_hessian_in_leaf=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 4.7273 (4.5322)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 4.7281 (4.5338)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(KRR)\n",
    "# print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 2.3446 (0.3917)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 2.5654 (0.6163)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM score: 2.4145 (0.5321)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models=(lasso, ENet, GBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(averaged_models)\n",
    "print(\"Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_xgb),\n",
    "                                                 meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6197091791981342\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models.fit(train.values, y)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
    "print(rmsle(y, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0532774206676123\n"
     ]
    }
   ],
   "source": [
    "model_xgb.fit(train, y)\n",
    "xgb_train_pred = model_xgb.predict(train)\n",
    "xgb_pred = np.expm1(model_xgb.predict(test))\n",
    "print(rmsle(y, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8192734224504923\n"
     ]
    }
   ],
   "source": [
    "model_lgb.fit(train, y)\n",
    "lgb_train_pred = model_lgb.predict(train)\n",
    "lgb_pred = np.expm1(model_lgb.predict(test.values))\n",
    "print(rmsle(y, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "1.5816884415147248\n"
     ]
    }
   ],
   "source": [
    "print('RMSLE score on train data:')\n",
    "print(rmsle(y,stacked_train_pred*0.70 +\n",
    "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = sub['id']\n",
    "df_sub['revenue'] = ensemble\n",
    "df_sub.to_csv(\"submission_ens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "K = 10\n",
    "fold = list(KFold(K, shuffle=True, random_state=SEED).split(train))\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(trn_X, trn_y, val_X, val_y, test, verbose):\n",
    "    \n",
    "    params = {'objective': 'reg:linear',\n",
    "              'eta': 0.01, \n",
    "              'max_depth': 5,\n",
    "              'subsample': 0.6,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'eval_metrics': 'rmse',\n",
    "              'seed': SEED,\n",
    "              'silent': True}\n",
    "    \n",
    "    record = dict()\n",
    "    \n",
    "    model = xgb.train(params, xgb.DMatrix(trn_X, trn_y), 1000,\n",
    "                      [(xgb.DMatrix(trn_X, trn_y), 'train'),\n",
    "                      (xgb.DMatrix(val_X, val_y), 'valid')],\n",
    "                      verbose_eval=verbose,\n",
    "                      early_stopping_rounds=200,\n",
    "                      callbacks=[xgb.callback.record_evaluation(record)])\n",
    "    \n",
    "    best_idx = np.argmin(np.array(record['valid']['rmse']))\n",
    "    \n",
    "    val_pred = model.predict(xgb.DMatrix(val_X), ntree_limit=model.best_ntree_limit)\n",
    "    test_pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n",
    "    \n",
    "    return {'val': val_pred, 'test': test_pred, 'error': record['valid']['rmse'][best_idx], 'importance': [i for k, i in model.get_score().items()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n",
    "\n",
    "    params = {'objective':'regression',\n",
    "         'num_leaves' : 40,\n",
    "         'min_data_in_leaf' : 20,\n",
    "         'max_depth' : 5,\n",
    "         'learning_rate': 0.01,\n",
    "         'feature_fraction': 0.8,\n",
    "         'bagging_freq': 1,\n",
    "         'bagging_fraction': 0.8,\n",
    "         'bagging_seed': SEED,\n",
    "         'metric': 'rmse',\n",
    "         'random_state' : SEED,\n",
    "         'verbosity': -1}\n",
    "\n",
    "    record = dict()\n",
    "    model = lgb.train(params\n",
    "                      , lgb.Dataset(trn_x, trn_y)\n",
    "                      , num_boost_round = 10000\n",
    "                      , valid_sets = [lgb.Dataset(val_x, val_y)]\n",
    "                      , verbose_eval = verbose\n",
    "                      , early_stopping_rounds = 200\n",
    "                      , callbacks = [lgb.record_evaluation(record)]\n",
    "                     )\n",
    "    best_idx = np.argmin(np.array(record['valid_0']['rmse']))\n",
    "\n",
    "    val_pred = model.predict(val_x, num_iteration = model.best_iteration)\n",
    "    test_pred = model.predict(test, num_iteration = model.best_iteration)\n",
    "    \n",
    "    return {'val': val_pred, 'test': test_pred, 'error': record['valid_0']['rmse'][best_idx], 'importance': model.feature_importance('gain')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def cat_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=10000,\n",
    "                                 learning_rate=0.01,\n",
    "                                 depth=5,\n",
    "                                 eval_metric='RMSE',\n",
    "                                 colsample_bylevel=0.7,\n",
    "                                 random_seed=SEED,\n",
    "                                 bagging_temperature=0.2,\n",
    "                                 metric_period=None,\n",
    "                                 early_stopping_rounds=200)\n",
    "                                  \n",
    "    model.fit(trn_x, trn_y,\n",
    "                 eval_set=(val_x, val_y),\n",
    "                 use_best_model=True,\n",
    "                 verbose=False)\n",
    "    \n",
    "    val_pred = model.predict(val_x)\n",
    "    test_pred = model.predict(test)\n",
    "    \n",
    "    return {'val': val_pred, 'test': test_pred, 'error': model.get_best_score()['validation_0']['RMSE'], 'importance': model.get_feature_importance()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold.    RMSE\n",
      "xgb model. 2.20369 (0m)\n",
      "lgb model. 2.16956 (0m)\n",
      "cat model. 2.24732 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.20686\n",
      "blend err. 2.19017\n",
      "\n",
      "2 fold.    RMSE\n",
      "xgb model. 2.60680 (0m)\n",
      "lgb model. 2.58602 (0m)\n",
      "cat model. 2.63044 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.60775\n",
      "blend err. 2.59303\n",
      "\n",
      "3 fold.    RMSE\n",
      "xgb model. 2.30970 (0m)\n",
      "lgb model. 2.29705 (0m)\n",
      "cat model. 2.31970 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.30882\n",
      "blend err. 2.29539\n",
      "\n",
      "4 fold.    RMSE\n",
      "xgb model. 2.29237 (0m)\n",
      "lgb model. 2.27811 (0m)\n",
      "cat model. 2.26792 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.27947\n",
      "blend err. 2.26810\n",
      "\n",
      "5 fold.    RMSE\n",
      "xgb model. 2.43281 (0m)\n",
      "lgb model. 2.44185 (0m)\n",
      "cat model. 2.37311 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.41592\n",
      "blend err. 2.38865\n",
      "\n",
      "6 fold.    RMSE\n",
      "xgb model. 2.33372 (0m)\n",
      "lgb model. 2.32472 (0m)\n",
      "cat model. 2.32285 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.32710\n",
      "blend err. 2.31444\n",
      "\n",
      "7 fold.    RMSE\n",
      "xgb model. 2.16152 (0m)\n",
      "lgb model. 2.14261 (0m)\n",
      "cat model. 2.21113 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.17176\n",
      "blend err. 2.15094\n",
      "\n",
      "8 fold.    RMSE\n",
      "xgb model. 2.32375 (0m)\n",
      "lgb model. 2.28427 (0m)\n",
      "cat model. 2.38708 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.33170\n",
      "blend err. 2.31786\n",
      "\n",
      "9 fold.    RMSE\n",
      "xgb model. 2.11210 (0m)\n",
      "lgb model. 2.07792 (0m)\n",
      "cat model. 2.09247 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.09417\n",
      "blend err. 2.08084\n",
      "\n",
      "10 fold.    RMSE\n",
      "xgb model. 2.37448 (0m)\n",
      "lgb model. 2.33866 (0m)\n",
      "cat model. 2.38388 (0m)\n",
      "---------------------------\n",
      "avg   err. 2.36567\n",
      "blend err. 2.35419\n",
      "\n",
      "fianl avg   err. 2.310920886193997\n",
      "fianl blend err. 2.2992864729479128\n"
     ]
    }
   ],
   "source": [
    "result_dict = dict()\n",
    "val_pred = np.zeros(train.shape[0])\n",
    "test_pred = np.zeros(test.shape[0])\n",
    "final_err = 0\n",
    "verbose = False\n",
    "\n",
    "for i, (trn, val) in enumerate(fold) :\n",
    "    print(i+1, \"fold.    RMSE\")\n",
    "    \n",
    "    trn_x = train.loc[trn, :]\n",
    "    trn_y = y[trn]\n",
    "    val_x = train.loc[val, :]\n",
    "    val_y = y[val]\n",
    "    \n",
    "    fold_val_pred = []\n",
    "    fold_test_pred = []\n",
    "    fold_err = []\n",
    "    \n",
    "    \n",
    "    start = datetime.now()\n",
    "    result = xgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val'])\n",
    "    fold_test_pred.append(result['test'])\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    \n",
    "    start = datetime.now()\n",
    "    result = lgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val'])\n",
    "    fold_test_pred.append(result['test'])\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"lgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    \n",
    "    start = datetime.now()\n",
    "    result = cat_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val'])\n",
    "    fold_test_pred.append(result['test'])\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"cat model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    \n",
    "      \n",
    "    val_pred[val] += np.mean(np.array(fold_val_pred), axis = 0)\n",
    "    test_pred += np.mean(np.array(fold_test_pred), axis = 0) / K\n",
    "    final_err += (sum(fold_err) / len(fold_err)) / K\n",
    "    \n",
    "    print(\"---------------------------\")\n",
    "    print(\"avg   err.\", \"{0:.5f}\".format(sum(fold_err) / len(fold_err)))\n",
    "    print(\"blend err.\", \"{0:.5f}\".format(np.sqrt(np.mean((np.mean(np.array(fold_val_pred), axis = 0) - val_y)**2))))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "print(\"fianl avg   err.\", final_err)\n",
    "print(\"fianl blend err.\", np.sqrt(np.mean((val_pred - y)**2)))\n",
    "\n",
    "boost_pred = np.expm1(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    prediction = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if model_type == 'sklearn':\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "                    verbose=1000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_squared_error(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Feb 19 14:58:41 2019\n",
      "[0]\ttrain-rmse:15.3476\tvalid_data-rmse:15.3523\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[500]\ttrain-rmse:1.75352\tvalid_data-rmse:2.44288\n",
      "[1000]\ttrain-rmse:1.41732\tvalid_data-rmse:2.41006\n",
      "Stopping. Best iteration:\n",
      "[1274]\ttrain-rmse:1.28846\tvalid_data-rmse:2.40256\n",
      "\n",
      "Fold 1 started at Tue Feb 19 14:58:53 2019\n",
      "[0]\ttrain-rmse:15.356\tvalid_data-rmse:15.3165\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[500]\ttrain-rmse:1.7981\tvalid_data-rmse:2.29623\n",
      "[1000]\ttrain-rmse:1.45894\tvalid_data-rmse:2.28266\n",
      "Stopping. Best iteration:\n",
      "[942]\ttrain-rmse:1.48938\tvalid_data-rmse:2.28064\n",
      "\n",
      "Fold 2 started at Tue Feb 19 14:59:02 2019\n",
      "[0]\ttrain-rmse:15.3707\tvalid_data-rmse:15.258\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[500]\ttrain-rmse:1.75365\tvalid_data-rmse:2.41352\n",
      "[1000]\ttrain-rmse:1.42883\tvalid_data-rmse:2.38623\n",
      "[1500]\ttrain-rmse:1.19827\tvalid_data-rmse:2.38323\n",
      "[2000]\ttrain-rmse:1.0171\tvalid_data-rmse:2.38284\n",
      "Stopping. Best iteration:\n",
      "[1826]\ttrain-rmse:1.07741\tvalid_data-rmse:2.38095\n",
      "\n",
      "Fold 3 started at Tue Feb 19 14:59:18 2019\n",
      "[0]\ttrain-rmse:15.3399\tvalid_data-rmse:15.3833\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[500]\ttrain-rmse:1.77846\tvalid_data-rmse:2.27713\n",
      "[1000]\ttrain-rmse:1.43629\tvalid_data-rmse:2.2455\n",
      "[1500]\ttrain-rmse:1.19537\tvalid_data-rmse:2.24509\n",
      "Stopping. Best iteration:\n",
      "[1396]\ttrain-rmse:1.23829\tvalid_data-rmse:2.24333\n",
      "\n",
      "Fold 4 started at Tue Feb 19 14:59:31 2019\n",
      "[0]\ttrain-rmse:15.3277\tvalid_data-rmse:15.4324\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[500]\ttrain-rmse:1.78406\tvalid_data-rmse:2.28803\n",
      "Stopping. Best iteration:\n",
      "[641]\ttrain-rmse:1.67091\tvalid_data-rmse:2.28015\n",
      "\n",
      "CV mean score: 2.3175, std: 0.0625.\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'eta': 0.01,\n",
    "              'objective': 'reg:linear',\n",
    "              'max_depth': 5,\n",
    "              'subsample': 0.8,\n",
    "              'colsample_bytree': 0.8,\n",
    "              'eval_metric': 'rmse',\n",
    "              'seed': 11,\n",
    "              'silent': True}\n",
    "\n",
    "oof_xgb, prediction_xgb = train_model(train, test, y, params=xgb_params, model_type='xgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Feb 19 15:00:58 2019\n",
      "Fold 1 started at Tue Feb 19 15:00:59 2019\n",
      "Fold 2 started at Tue Feb 19 15:00:59 2019\n",
      "Fold 3 started at Tue Feb 19 15:01:00 2019\n",
      "Fold 4 started at Tue Feb 19 15:01:00 2019\n",
      "CV mean score: 6.1487, std: 6.5981.\n"
     ]
    }
   ],
   "source": [
    "model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n",
    "\n",
    "oof_ridge, prediction_ridge = train_model(train.fillna(0).values, test.fillna(0).values, y, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Feb 19 15:07:13 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\tvalid_0's rmse: 1.36697\tvalid_1's rmse: 2.43312\n",
      "Early stopping, best iteration is:\n",
      "[831]\tvalid_0's rmse: 1.46307\tvalid_1's rmse: 2.43068\n",
      "Fold 1 started at Tue Feb 19 15:07:16 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[425]\tvalid_0's rmse: 1.80487\tvalid_1's rmse: 2.26551\n",
      "Fold 2 started at Tue Feb 19 15:07:17 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\tvalid_0's rmse: 1.36581\tvalid_1's rmse: 2.38286\n",
      "Early stopping, best iteration is:\n",
      "[881]\tvalid_0's rmse: 1.43339\tvalid_1's rmse: 2.381\n",
      "Fold 3 started at Tue Feb 19 15:07:19 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\tvalid_0's rmse: 1.40697\tvalid_1's rmse: 2.23667\n",
      "Early stopping, best iteration is:\n",
      "[908]\tvalid_0's rmse: 1.45622\tvalid_1's rmse: 2.23403\n",
      "Fold 4 started at Tue Feb 19 15:07:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[469]\tvalid_0's rmse: 1.74762\tvalid_1's rmse: 2.25303\n",
      "CV mean score: 2.3129, std: 0.0782.\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 7,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "oof_lgb, prediction_lgb = train_model(train,test, y, params=params, model_type='lgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_pred = np.expm1((prediction_lgb + prediction_xgb + prediction_ridge) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.75 + boost_pred*0.15 + blend_pred*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = sub['id']\n",
    "df_sub['revenue'] = ensemble\n",
    "df_sub.to_csv(\"submission_ens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
